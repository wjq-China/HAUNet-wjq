{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S_CEMBlock(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2,num_heads=3, FFN_Expand=2, drop_out_rate=0.):\n",
    "        \"\"\"S-CEM模块\n",
    "\n",
    "        Args:\n",
    "            c (_type_): _description_\n",
    "            DW_Expand (int, optional): _description_. Defaults to 2.\n",
    "            num_heads (int, optional): _description_. Defaults to 3.\n",
    "            FFN_Expand (int, optional): _description_. Defaults to 2.\n",
    "            drop_out_rate (_type_, optional): _description_. Defaults to 0..\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.qkv = nn.Conv2d(c, c * 3, kernel_size=1)\n",
    "        self.qkv_dwconv = nn.Conv2d(c * 3, c * 3, kernel_size=3, stride=1, padding=1, groups=c * 3)\n",
    "\n",
    "        self.project_out = nn.Conv2d(c, c, kernel_size=1)\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.project_out2 = nn.Conv2d(c, c, kernel_size=1)\n",
    "        self.temperature2 = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        # SimpleGate\n",
    "        self.sg = SimpleGate()\n",
    "\n",
    "        ffn_channel = FFN_Expand * c\n",
    "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1,\n",
    "                               bias=True)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1,\n",
    "                               groups=1, bias=True)\n",
    "\n",
    "        self.norm1 = LayerNorm2d(c)\n",
    "        self.norm2 = LayerNorm2d(c)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "\n",
    "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.beta2 = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        x = self.norm1(x)  # layernorm\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q, k, v = qkv.chunk(3, dim=1)  # 沿1轴切分为3块\n",
    "\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)  # 通道注意力\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        qs = q.clone().permute(0, 1, 3, 2)  # 空间注意力\n",
    "        ks = k.clone().permute(0, 1, 3, 2)\n",
    "        vs = v.clone().permute(0, 1, 3, 2)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn=self.relu(attn)\n",
    "        attn = self.softmax(attn)\n",
    "\n",
    "        outc = (attn @ v)  # 通道注意力的输出\n",
    "\n",
    "        qs = torch.nn.functional.normalize(qs, dim=-1)\n",
    "        ks = torch.nn.functional.normalize(ks, dim=-1)\n",
    "\n",
    "        attns = (qs @ ks.transpose(-2, -1)) * self.temperature2\n",
    "        attns=self.relu(attns)\n",
    "        attns = self.softmax(attns)\n",
    "        outs = (attns @ vs)\n",
    "        outs = outs.permute(0, 1, 3, 2)  # 空间注意力的输出\n",
    "\n",
    "        outc = rearrange(outc, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "        outs = rearrange(outs, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        xc = self.project_out(outc)  # \n",
    "        xc = self.dropout1(xc)\n",
    "        xs = self.project_out2(outs)\n",
    "        xs = self.dropout1(xs)\n",
    "\n",
    "        y = inp + xc * self.beta+ xs * self.beta2  # 加和\n",
    "\n",
    "        x = self.conv4(self.norm2(y))\n",
    "        x = self.sg(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return y + x * self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
